# **Benchmaker-Lite â€” FastAPI Benchmarking & Observability Pipeline**

[ğŸ”— Live landing page:](https://slevinas.github.io/benchmaker-lite/)

<p align="left">
  <img src="https://img.shields.io/badge/Python-3.11+-blue.svg" />
  <img src="https://img.shields.io/badge/FastAPI-Instrumented%20with%20OTel-009688.svg" />
  <img src="https://img.shields.io/badge/ClickHouse-Analytics%20DB-yellow.svg" />
  <img src="https://img.shields.io/badge/OpenTelemetry-Collector%20Pipeline-purple.svg" />
  <img src="https://img.shields.io/badge/Asyncio-Concurrency%20Testing-orange.svg" />
  <img src="https://img.shields.io/badge/Docker-Compose-green.svg" />
  <img src="https://img.shields.io/badge/Status-Active%20Project-brightgreen.svg" />
</p>

## Table of Contents

- [Overview](#overview)
- [Architecture](#architecture-overview)
- [Components](#ï¸-components)
  - [FastAPI Benchmark Target](#1-api--fastapi-benchmark-target)
  - [Async Benchmark Client](#2-benchmark_client--async-python-runner)
  - [ClickHouse Layer](#3-clickhouse--db-layer)
  - [OpenTelemetry Collector](#4-otel--opentelemetry-collector)
- [Schema](#schema)
- [Running the Stack](#-running-the-entire-pipeline)
  - [Start Services](#1-start-dependencies)
  - [Run a Benchmark](#2-run-a-benchmark)
  - [Query Results](#3-query-results-manually)
- [Why This Project Exists](#-why-this-project)
- [Project Internals](#-project-internals)
- [Roadmap](#-roadmap)
- [License](#license)

---

#### Overview

Benchmaker-Lite is a **fully containerized benchmarking and observability system** built around:

- **FastAPI** (instrumented with OpenTelemetry)
- **OpenTelemetry Collector** (file + debug exporters)
- **ClickHouse** (analytics DB)
- **Async Python benchmark client** (httpx + asyncio)
- **Custom ClickHouse client** (env-driven config, JSONEachRow inserts)

It demonstrates real-world DevOps, observability, performance engineering, and backend automation patterns.

---

# **Architecture Overview**

> The benchmark output you see below is generated by this exact pipeline: async client â†’ FastAPI target â†’ OTEL Collector â†’ ClickHouse.

```
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚   Benchmark Client    â”‚
                      â”‚  (asyncio + httpx)    â”‚
                      â”‚  generate load / run  â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
                     POST /api/vector/add (FastAPI)
                                  â”‚
                                  â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚        FastAPI Service              â”‚
                â”‚  - Vector-add endpoint              â”‚
                â”‚  - OTel instrumentation (SDK)       â”‚
                â”‚  - Emits traces & metrics           â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚  OTLP/gRPC
                                  â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚        OpenTelemetry Collector        â”‚
               â”‚  - Receives telemetry                 â”‚
               â”‚  - Batching processor                 â”‚
               â”‚  - Exports: file(traces), file(metrics) â”‚
               â”‚  - Debug exporter (stdout)            â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                              ETL / Ingest
                                  â”‚
                                  â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚               ClickHouse                â”‚
              â”‚  - schema: benchmark_results           â”‚
              â”‚  - JSONEachRow inserts                 â”‚
              â”‚  - analytical queries (p95, p99, etc.) â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# âš™ï¸ **Components**

### **1. `/api` â€” FastAPI Benchmark Target**

- Implements `/api/vector/add`
- Instrumented with OpenTelemetry SDK
- Emits traces & metrics to OTEL Collector
- Designed for load-generation & latency measurement

### **2. `/benchmark_client` â€” Async Python Runner**

- Uses `httpx.AsyncClient` + `asyncio`
- Launches concurrent workers
- Computes:

  - avg latency
  - p95, p99
  - min/max

- Stores structured results into ClickHouse
- Can fetch and display recent benchmark history

### **3. `/clickhouse` â€” DB Layer**

Includes:

- `client.py` (custom ClickHouse HTTP client)
- `init.sql` (schema definitions)
- Config-driven table design for benchmark analytics

Table:

```sql
CREATE TABLE benchmark_results (
    timestamp       DateTime DEFAULT now(),
    endpoint        String,
    avg_latency     Float64,
    p95_latency     Float64,
    p99_latency     Float64,
    min_latency     Float64,
    max_latency     Float64,
    total_requests  UInt32
) ENGINE = MergeTree()
ORDER BY (timestamp, endpoint);
```

### **4. `/otel` â€” OpenTelemetry Collector**

- Receives FastAPI telemetry
- Writes traces/metrics to local file
- Debug exporter for introspection

---

# ğŸ³ **Running the Entire Pipeline**

### **1. Start dependencies**

```bash
docker-compose up --build
```

This launches:

- ClickHouse
- OTEL Collector
- FastAPI benchmark service

### **2. Run a benchmark**

```bash
python -m benchmark_client.run_benchmark
```

Output:

```
=== Benchmark Summary ===
count: 500
avg: 0.0103s
p95: 0.0199s
p99: 0.0642s
min: 0.0041s
max: 0.0888s

Saving summary to ClickHouse...
Saved.

Fetching recent results...
timestamp                  avg_latency     p95_latency   total_requests
-----------------------------------------------------------------------
2025-12-05 13:24:33        0.0103           0.0199         500
...
```

### **3. Query results manually**

```sql
SELECT *
FROM benchmark_results
ORDER BY timestamp DESC
LIMIT 10;
```

---

# ğŸ“¦ **Why This Project?**

This system simulates a **real observability + benchmarking pipeline**:

- Microservice exposing a performance-critical endpoint
- Telemetry instrumentation & OTEL ingestion
- Async load generation (multi-worker)
- Persistance to ClickHouse for analytics
- Queryable history of performance metrics

It showcases:

- DevOps automation
- Distributed tracing
- Telemetry pipelines
- Backend benchmarking
- Async Python tooling
- ClickHouse data engineering
- Docker Compose orchestration
- Practical, production-style architecture

---

# **ğŸ§© Project Internals**

This project is structured around three core components:

---

## **1. FastAPI Service (`/api`)**

- Exposes benchmark endpoints (e.g., `/vector/add`).
- Instrumented with **OpenTelemetry SDK**.
- Sends traces/metrics to the **OpenTelemetry Collector** via OTLP/gRPC.
- Emits latency events that the benchmark client measures.

---

## **2. Benchmark Client (`/benchmark_client`)**

- Async load generator using **httpx + asyncio**.
- Computes:

  - average latency
  - p95 / p99 latency
  - min/max latency

- Saves structured summaries into ClickHouse using the **ClickHouseClient**.
- Optionally ETLs OTEL trace files into analytical tables.

---

## **3. ClickHouse Layer (`/clickhouse`)**

- Contains:

  - database schema (`init.sql`)
  - **ClickHouseClient** (Python wrapper for HTTP API)
  - future ETL tools for traces/metrics

- Central store for benchmark analytics.

Example summary row:

| total_requests | avg_latency | p95_latency | timestamp |
| -------------- | ----------- | ----------- | --------- |
| 500            | 0.008       | 0.021       | now()     |

---

## ğŸ“Œ Roadmap

**Near-term (1â€“2 weeks)**

- [ ] Add latency histograms via OTEL Metrics SDK
- [ ] Add ClickHouse aggregation views (p95, p99, percentiles)
- [ ] Add automatic CSV export for benchmark reports
- [ ] Add Docker image publishing (GHCR)

**Mid-term (1 month)**

- [ ] Add distributed load testing mode (multiple workers)
- [ ] Store OTEL traces in ClickHouse for drill-down analysis
- [ ] Add Grafana dashboards (ClickHouse plugin)

**Long-term (future)**

- [ ] Support multiple benchmark targets
- [ ] Add gRPC benchmark target
- [ ] Add synthetic failure injection + resiliency testing mode
- [ ] Convert project into reusable Python package (`pip install benchmaker-lite`)

---

## ğŸ“Š Example Benchmark Output

The benchmark client runs a high-concurrency async workload against the FastAPI
endpoint (`/api/vector/add`), computes latency statistics (avg / p95 / p99 / min / max),
and writes the results into ClickHouse. Below is an example benchmark run:

![Benchmark Summary](docs/images/Screenshot-pipeline-execution-results.png)

---

## ğŸ“‚ Stored Benchmark Results in ClickHouse

After each benchmark cycle, results are stored using `JSONEachRow` inserts.
This enables fast analytical queries against past runs:

![ClickHouse Results](docs/images/screenshot-benchmar_results_table_query.png)

---

## ğŸ“„ License

Released under the MIT License.  
See the `LICENSE` file for details.
