# **ğŸ“Š Benchmaker-Lite â€” FastAPI Benchmarking + Observability Pipeline**

A small but complete DevOps + Observability project demonstrating:

- FastAPI service instrumented with **OpenTelemetry**
- Async benchmark client using **httpx + asyncio**
- OpenTelemetry Collector receiving traces/metrics
- Exporting telemetry into **ClickHouse**
- Docker Compose orchestration across all components
- Benchmark storage, analysis queries, and end-to-end reproducibility


---

### 1ï¸âƒ£ Observability Pipeline Diagram

```mermaid
flowchart LR
    subgraph App["FastAPI App (benchmaker-lite)"]
        A[Incoming HTTP Requests]
        B[Business Logic / Vector Ops]
        C[OTel SDK<br/>Traces & Metrics]
        A --> B --> C
    end

    C --> D[OTLP Exporter]

    subgraph Collector["OTel Collector"]
        D --> E[Receivers (otlp)]
        E --> F[Processors<br/>(batch, attributes, transform)]
        F --> G[Exporters<br/>(clickhouse / file)]
    end

    G --> H[Fluent Bit (optional)]
    H --> I[ClickHouse]

    I --> J[Analytics / Queries<br/>(latency, errors, throughput)]
```

### 2ï¸âƒ£ Benchmaker-lite System Diagram

```mermaid
flowchart LR
    U[Developer / Engineer] --> CLI[Benchmark CLI (Python)]
    CLI -->|async httpx| BM[Benchmark Runner]

    subgraph Runner["Benchmark Runner"]
        BM -->|N concurrent requests| API[(FastAPI Service)]
        BM --> MET[Benchmark Summary<br/>(latency stats, errors)]
    end

    API --> OTel[OTel SDK]
    OTel --> COL[OTel Collector]
    COL --> CH[ClickHouse]

    MET --> CH
    CH --> Q[Analysis / Visualization<br/>(SQL queries, notebooks)]
```
---

## **ğŸš€ Features**

### **Benchmarking**

- Async Python client (`httpx`) sending thousands of requests
- Measures: p95/p99 latency, min/max, total throughput
- Stores summary into ClickHouse (`benchmark_results` table)

### **Observability**

- Automatic OTEL instrumentation of FastAPI routes
- Real traces emitted per benchmark request
- Collector pipelines exporting to ClickHouse
- Schema auto-created (`otel_traces`, `otel_metrics_*`)

### **DevOps**

- Multi-container Docker Compose setup
- Resilient startup (collector waits for ClickHouse)
- Configuration via environment variables
- All services run locally with one command:

```bash
docker-compose up --build
```

---

## **ğŸ§± Architecture  Diagram**


```
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚     Benchmark Client    â”‚
                        â”‚  (async httpx load gen) â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
                         HTTP Requests (load)
                                     â”‚
                                     â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚             FastAPI              â”‚
                   â”‚  - /vector/add benchmark route   â”‚
                   â”‚  - /health                       â”‚
                   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                   â”‚  OTEL SDK Instrumentation        â”‚
                   â”‚  - Traces emitted per request    â”‚
                   â”‚  - Metrics (latency counters)    â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚ OTLP (gRPC/HTTP)
                                      â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚  OpenTelemetry Collector        â”‚
                        â”‚---------------------------------â”‚
                        â”‚ Receivers:                      â”‚
                        â”‚   - otlp/http                   â”‚
                        â”‚   - otlp/grpc                   â”‚
                        â”‚ Processors:                     â”‚
                        â”‚   - batch                       â”‚
                        â”‚   - (optional transforms)       â”‚
                        â”‚ Exporters:                      â”‚
                        â”‚   - debug                       â”‚
                        â”‚   - clickhouse (traces/metrics) â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚ TCP (9000)
                                           â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚            ClickHouse             â”‚
                         â”‚-----------------------------------â”‚
                         â”‚  Database: otel                   â”‚
                         â”‚  Tables created automatically:    â”‚
                         â”‚    - otel_traces                  â”‚
                         â”‚    - otel_logs                    â”‚
                         â”‚    - otel_metrics_*               â”‚
                         â”‚                                   â”‚
                         â”‚  Benchmark Results (manual insert)â”‚
                         â”‚  stored into: default.bench...    â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
## **Architecture Overview**
### **What the architecture demonstrates**

- Observability pipeline from code â†’ telemetry â†’ storage
- Automation pipeline for benchmarking, latency measurement
- Real-world DevOps stack (ClickHouse, OTEL Collector, Docker Compose)
- Async load testing and benchmarking framework you built
- Experience in distributed tracing, schema creation, and telemetry exports

---

---

## **ğŸ§ª Running a Benchmark**

```bash
python -m benchmark_client.run_benchmark
```

Example output:

```
Benchmark Summary
Total requests: 5000
Avg latency: 0.0080s
p95 latency: 0.021s
p99 latency: 0.041s
Min latency: 0.0017s
Max latency: 0.0876s
```

---

## **ğŸ“¦ Docker Stack**

| Service              | Description                                          |
| -------------------- | ---------------------------------------------------- |
| **FastAPI app**      | Handles `/vector/add` requests and emits OTEL traces |
| **OTEL Collector**   | Receives OTLP data and exports to ClickHouse         |
| **ClickHouse**       | Stores traces, metrics, and benchmark results        |
| **Benchmark Client** | Generates concurrent load using asyncio              |

---

## **ğŸ” Querying Telemetry in ClickHouse**

### Recent traces:

```sql
SELECT Timestamp, ServiceName, SpanName
FROM otel_traces
ORDER BY Timestamp DESC
LIMIT 20;
```

### p95 latency (example for deeper analysis):

```sql
SELECT
  quantile(0.95)(Duration) AS p95_latency
FROM otel_traces
WHERE ServiceName = 'benchmaker-lite-api';
```

### Benchmark summaries:

```sql
SELECT *
FROM benchmark_results
ORDER BY timestamp DESC
LIMIT 10;
```

---


























# OTEL Collector Configuration

This folder contains the configuration for the **OpenTelemetry Collector** used
by `benchmaker-lite`.

- `collector-config.yaml` â€“ minimal pipeline:
  - `otlp` receiver (gRPC + HTTP)
  - `batch` processor
  - `debug` exporter (prints spans to stdout)

In a more advanced setup, you could:

- Export traces/metrics directly into ClickHouse or another backend.
- Add processors for attribute enriching, sampling, or redaction.
- Add metrics/log pipelines alongside traces.
